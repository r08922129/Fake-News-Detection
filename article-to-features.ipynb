{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "surprised-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.FeatureConfig import FeatureConfig\n",
    "from src.utils.FeatureExtractor import FeatureExtractor\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "\n",
    "def loadDataset(path):\n",
    "    data = []\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        with open(os.path.join(path, file)) as f:\n",
    "            data.append(f.read())\n",
    "    return data\n",
    "\n",
    "def dataToFeatures(data, featureExtractor):\n",
    "    \n",
    "    features = []\n",
    "    for doc in data:\n",
    "        features.append(featureExtractor.extract())\n",
    "\n",
    "def kFoldsValidation(X, y, k=10, shuffle=True):\n",
    "    '''\n",
    "    Args:\n",
    "        X: a n * m matrix. n is the number of samples and m is the dimension of features\n",
    "        y: a ndarray. with shape (n, )\n",
    "    \n",
    "    Return:\n",
    "        a list of accuracy\n",
    "    '''\n",
    "    numberOfSample = X.shape[0]\n",
    "    batchSize = numberOfSample//k\n",
    "    X = np.concatenate([y.reshape(-1, 1), X], axis = 1)\n",
    "    np.random.shuffle(X)\n",
    "    \n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto', C=10))\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        testingIndices = np.arange(i*batchSize, (i+1)*batchSize)\n",
    "        trainingIndices = np.delete(np.arange(numberOfSample), testingIndices)\n",
    "        \n",
    "        trainX = X[trainingIndices, 1:]\n",
    "        trainY = X[trainingIndices, 0]\n",
    "        clf.fit(trainX, trainY)\n",
    "        \n",
    "        testX = X[testingIndices, 1:]\n",
    "        testY = X[testingIndices, 0]\n",
    "        out.append((clf.predict(testX) == testY).sum()/testY.shape[0])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "stable-delta",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped BOWExtractor.\n",
      "Dropped GIExtractor.\n",
      "Dropped ProductionExtractor.\n"
     ]
    }
   ],
   "source": [
    "config = FeatureConfig(pathToDataset='data/fakeNewsDatasets/fakeNewsDataset/',\n",
    "                       pathToGI='inquirerbasic.xls',\n",
    "                       collectPosFromCorpus=False,\n",
    "                       collectProductionFromCorpus=False,\n",
    "                       )\n",
    "featureExtractor = FeatureExtractor(config)\n",
    "featureExtractor.load(\"save/state.pkl\")\n",
    "featureExtractor.drop(\"BOWExtractor\")\n",
    "featureExtractor.drop(\"GIExtractor\")\n",
    "featureExtractor.drop(\"ProductionExtractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "funded-simon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GIExtractor',\n",
       " 'PosExtractor',\n",
       " 'ProductionExtractor',\n",
       " 'ReadabilityExtractor',\n",
       " 'QuantityExtractor',\n",
       " 'SentimentExtractor']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureExtractor.extractorName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "indoor-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "legitData = loadDataset('data/fakeNewsDatasets/fakeNewsDataset/legit/')\n",
    "fakeData = loadDataset('data/fakeNewsDatasets/fakeNewsDataset/fake/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "practical-selling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '3'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '8'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '2'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '1'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '5'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '0'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '4'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '7'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '6'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '£'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '9'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '–'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'á'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '‑'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'é'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'à'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '�'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ã'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ü'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n"
     ]
    }
   ],
   "source": [
    "legitFeatures = featureExtractor.extract(legitData)\n",
    "fakeFeatures = featureExtractor.extract(fakeData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "hazardous-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([legitFeatures, fakeFeatures], axis = 0)\n",
    "y = np.array([0] * len(legitFeatures) + [1] * len(fakeFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "varying-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = kFoldsValidation(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "varied-attraction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.712962962962963"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(acc)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "terminal-church",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PosExtractor',\n",
       " 'ReadabilityExtractor',\n",
       " 'QuantityExtractor',\n",
       " 'SentimentExtractor']"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureExtractor.extractorName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "weird-africa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.utils.PosExtractor.PosExtractor object at 0x7f3cfcd86790> Accuracy: 0.64375.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '3'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '8'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '2'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '1'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '5'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '0'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '4'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '7'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '6'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '£'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '9'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '–'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'á'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '‑'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'é'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'à'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '�'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ã'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n",
      "/home/allen/anaconda3/envs/syntax/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ü'\n",
      "  \" assigning as vowel: '{}'\".format(c))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.utils.ReadabilityExtractor.ReadabilityExtractor object at 0x7f3cfccd9610> Accuracy: 0.5229166666666666.\n",
      "<src.utils.QuantityExtractor.QuantityExtractor object at 0x7f3cfccd9650> Accuracy: 0.5374999999999999.\n",
      "<src.utils.SentimentExtractor.SentimentExtractor object at 0x7f3cfccd9690> Accuracy: 0.5020833333333333.\n"
     ]
    }
   ],
   "source": [
    "for extractor in featureExtractor.extractors:\n",
    "    legitFeatures = extractor.extract(legitData)\n",
    "    fakeFeatures = extractor.extract(fakeData)\n",
    "    X = np.concatenate([legitFeatures, fakeFeatures], axis = 0)\n",
    "    y = np.array([0] * len(legitFeatures) + [1] * len(fakeFeatures))\n",
    "    acc = kFoldsValidation(X, y)\n",
    "    print(\"{} Accuracy: {}.\".format(extractor, sum(acc)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "signal-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'documents', 'second']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "english-latest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.722056  , 0.69183461,\n",
       "        0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['He is is one of a dog.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "contrary-mirror",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "eligible-confusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 10:52:40 INFO: Writing properties to tmp file: corenlp_server-cc936bcc5b364f5f.props\n",
      "2021-03-17 10:52:40 INFO: Starting server with command: java -Xmx16G -cp /home/allen/stanza_corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-cc936bcc5b364f5f.props -annotators parse -preload -outputFormat serialized\n"
     ]
    }
   ],
   "source": [
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "with CoreNLPClient(\n",
    "    annotators=['parse'],\n",
    "    timeout=30000,\n",
    "    memory='16G') as client:\n",
    "    text = 'This Trump is the second document and documents.'\n",
    "    annotate = client.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "junior-times",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 18.6MB/s]                    \n",
      "2021-03-17 10:57:32 INFO: Downloading default packages for language: en (English)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/default.zip: 100%|██████████| 411M/411M [01:16<00:00, 5.38MB/s] \n",
      "2021-03-17 10:58:53 INFO: Finished downloading models and saved to /home/allen/stanza_resources.\n",
      "2021-03-17 10:58:53 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-03-17 10:58:53 INFO: Use device: gpu\n",
      "2021-03-17 10:58:53 INFO: Loading: tokenize\n",
      "2021-03-17 10:58:55 INFO: Loading: ner\n",
      "2021-03-17 10:58:55 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: Chris\tner: B-PERSON\n",
      "token: Manning\tner: E-PERSON\n",
      "token: teaches\tner: O\n",
      "token: at\tner: O\n",
      "token: Stanford\tner: B-ORG\n",
      "token: University\tner: E-ORG\n",
      "token: .\tner: O\n",
      "token: He\tner: O\n",
      "token: lives\tner: O\n",
      "token: in\tner: O\n",
      "token: the\tner: B-LOC\n",
      "token: Bay\tner: I-LOC\n",
      "token: Area\tner: E-LOC\n",
      "token: .\tner: O\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, ner')\n",
    "doc = nlp(\"Chris Manning teaches at Stanford University. He lives in the Bay Area.\")\n",
    "print(*[f'token: {token.text}\\tner: {token.ner}' for sent in doc.sentences for token in sent.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "continent-medicine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  {\n",
       "    \"id\": 3,\n",
       "    \"text\": \"teaches\",\n",
       "    \"misc\": \"start_char=14|end_char=21\",\n",
       "    \"ner\": \"O\"\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0].tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "fresh-mistake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E-PERSON'"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0].tokens[1].ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "robust-credits",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"O\"==\"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-nightlife",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
